# Test-MD
Este repositorio solo contendra un README.md muy chulo (Copiado del gran Fran Asensi -> Dynam1co)

## Comenzando üöÄ

El enunciado del ejercicio puede encontrarse [aqu√≠](Enunciado.md)

### Pre-requisitos üìã
- Java 8
- [IntelliJ](https://www.jetbrains.com/idea/) IDE para programar en Scala
- [Scala 2.11.12](https://www.scala-lang.org/)
- [Apache Kafka 2.12-2.3.0](https://kafka.apache.org/)
- [sbt](https://www.scala-sbt.org/)

### Instalaci√≥n üîß

El proyecto se ha desarrollado y probado usando el sistema operativo Mac OS Catalina 10.15

Las librer√≠as usadas dentro del proyecto son las siguientes:

- Spark Core
- Spark Sql
- Spark Streaming
- Spark Streaming Kafka
- Spark Sql Kafka

### Configuraci√≥n ‚öôÔ∏è
Se ha usado **sbt** como gestor de dependencias. Dentro del archivo build.sbt se encuentran las librer√≠as necesarias para el correcto funcionamiento del proyecto.

Dentro de la carpeta **Resources** se encuentra el fichero lista_negra.dat que contiene las palabras consideradas como "negativas".

Tambi√©n dentro de la carpeta **Resources** se encuentran los csv con la informaci√≥n de dispositivos, mensajes y usuarios. Los Ficheros son los siguientes: 

- **mensajes.csv** con los mensajes capturados por los diferentes dispositivos IoT.
- **dispositivos.csv** con la informaci√≥n de los diferentes dispositivos
- **usuarios.csv** con todos los usuarios de Celebram
- **palabras_excluir.dat** este fichero contiene palabaras que no se tienen en cuenta para el estudio (conjunciones, art√≠culos y preposiciones)

## Deployment üì¶

Al ejecutarse la aplicaci√≥n (fichero Main.scala), se cargar√° autom√°ticamente los csv de Usuarios y dispositivos IoT que estar√°n guardados en los csv correspondientes en la carpeta **Resources**.

Despu√©s se iniciar√° la espera de mensajes desde Kafka, en este paso tendremos que ir a la consola de Kafka y ejecutar el siguiente comando para cargar el csv con los mensajes:

```bash
bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test < ../../Proyectos_software/KC_Practica_BigData_Processing/src/main/resources/mensajes.csv
```

El sistema leer√° el fichero de palabras exclu√≠das (art√≠culos, preposiciones,.. ect) y no tendr√° en cuenta dichas palabras para el an√°lisis de los resultados.

Cuando acabe la ventana (para las pruebas se ha establecido en 10 segundos) se mostrar√° el resultado con las 10 palabras m√°s usadas, tambi√©n se comprobar√° si la palabra m√°s repetida coincide con alguna de las plabras de la lista negra, en ese caso, se mostrar√° un aviso por consola.

## Obtenci√≥n de mensajes üñ•Ô∏è

Para la obtenci√≥n de los mensajes, he creado un proyecto a parte en Scala que descarga Tweets en streaming y los almacena en un fichero de texto. Las librer√≠as usadas han sido:

- Spark Streaming
- Spark Streaming Twitter
- twitter4j



## Aclaraciones ‚úèÔ∏è

Al esquema de los mensajes he decidido incluir un nuevo campo correspondiente al ID del dispositivo IoT.

Al esquema de usuarios le he cambiado el tipo de dato a Int porque me parece m√°s √≥ptimo a nivel de BBDD.

A la hora de tratar los mensajes, se usa una funci√≥n para desencriptarlos y no se tienen en cuenta las preposiciones, conjunciones ni art√≠culos.

No he sido capaz de recuperar el timestamp, tampoco de obtener el top 10 porque al ser streaming da error. Al hacer la comprobaci√≥n contra la lista negra, tambi√©n me da error al hacer un "count" para comprobar si existe alguna palabra en dicha lista negra.

## Expresiones de Gratitud üéÅ

* Comenta a otros sobre este proyecto üì¢
* Da las gracias p√∫blicamente ü§ì
* S√≠gueme en <a href="https://twitter.com/AsensiFj">Twitter</a> üê¶
